{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Structured Functional Representations for Video Game Opinions (Viggo Dataset)\n",
    "\n",
    "Here, I generate structured functional representations of video game opinions the three LLMs: \n",
    "1. [mistralai/mixtral-8x7b-instruct-v0.1](https://replicate.com/mistralai/mixtral-8x7b-instruct-v0.1)\n",
    "2. [meta/meta-llama-3-70b-instruct](https://replicate.com/meta/meta-llama-3-70b-instruct)\n",
    "3. [meta/meta-llama-3-8b-instruct](https://replicate.com/meta/meta-llama-3-8b-instruct)\n",
    "\n",
    "I use the [Viggo dataset](https://huggingface.co/datasets/GEM/vigg). I make use of [Replicate](https://replicate.com/) APIs to generate responses from these models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset, Dataset\n",
    "import json\n",
    "import replicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess the Viggo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "print(\"Loading and preprocessing the dataset...\")\n",
    "\n",
    "# Load the Viggo dataset from the GEM benchmark\n",
    "dataset = load_dataset(\"GEM/viggo\")\n",
    "\n",
    "# Get the validation dataset\n",
    "val_dataset = dataset[\"validation\"]\n",
    "\n",
    "# Rename columns for consistency\n",
    "val_dataset = val_dataset.rename_columns({\n",
    "    \"meaning_representation\": \"attributes\", \n",
    "    \"target\": \"text\"\n",
    "})\n",
    "\n",
    "# Remove unnecessary columns\n",
    "val_dataset = val_dataset.remove_columns([\"gem_id\", \"references\"])\n",
    "\n",
    "# Delete the original full dataset to save memory\n",
    "del dataset\n",
    "\n",
    "print(\"Dataset loaded and preprocessed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a few examples\n",
    "print(\"\\nExamples:\")\n",
    "for i in range(2):\n",
    "    print(f\"\\nExample {i}\")\n",
    "    for key in ['text', 'attributes']: \n",
    "        print(f\"{key:12s}: {val_dataset[key][i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the prompt template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. \n",
    "This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute'].\n",
    "\n",
    "The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']. The order your list the attributes within the function must follow the order listed above. For example the 'name' attribute must always come before the 'exp_release_date' attribute, and so forth.\n",
    "\n",
    "For each attribute, fill in the corresponding value of the attribute within brackets. A couple of examples are below.\n",
    "\n",
    "Example 1)\n",
    "Sentence: Dirt: Showdown from 2012 is a sport racing game for the PlayStation, Xbox, PC rated E 10+ (for Everyone 10 and Older). It's not available on Steam, Linux, or Mac.\n",
    "Output: inform(name[Dirt: Showdown], release_year[2012], esrb[E 10+ (for Everyone 10 and Older)], genres[driving/racing, sport], platforms[PlayStation, Xbox, PC], available_on_steam[no], has_linux_release[no], has_mac_release[no])\n",
    "\n",
    "Example 2) \n",
    "Sentence: Were there even any terrible games in 2014?\n",
    "Output: request(release_year[2014], specifier[terrible])\n",
    "\n",
    "Example 3)\n",
    "Sentence: Adventure games that combine platforming and puzzles  can be frustrating to play, but the side view perspective is perfect for them. That's why I enjoyed playing Little Nightmares.\n",
    "Output: give_opinion(name[Little Nightmares], rating[good], genres[adventure, platformer, puzzle], player_perspective[side view])\n",
    "\n",
    "Example 4)\n",
    "Sentence: Since we're on the subject of games developed by Telltale Games, I'm wondering, have you played The Wolf Among Us?\n",
    "Output: recommend(name[The Wolf Among Us], developer[Telltale Games])\n",
    "\n",
    "Example 5) \n",
    "Sentence: Layers of Fear, the indie first person point-and-click adventure game?\n",
    "Output: confirm(name[Layers of Fear], genres[adventure, indie, point-and-click], player_perspective[first person])\t\n",
    "\n",
    "Example 6) \n",
    "Sentence: I bet you like it when you can play games on Steam, like Worms: Reloaded, right?\t\n",
    "Output: suggest(name[Worms: Reloaded], available_on_steam[yes])\n",
    "\n",
    "Example 7)\n",
    "Sentence: I recall you saying that you really enjoyed The Legend of Zelda: Ocarina of Time. Are you typically a big fan of games on Nintendo rated E (for Everyone)?\t\n",
    "Output: verify_attribute(name[The Legend of Zelda: Ocarina of Time], esrb[E (for Everyone)], rating[excellent], platforms[Nintendo])\n",
    "\n",
    "Example 8)\n",
    "Sentence: So what is it about the games that were released in 2005 that you find so excellent?\t\n",
    "Output: request_explanation(release_year[2005], rating[excellent])\n",
    "\n",
    "Example 9)\n",
    "Sentence: Do you think Mac is a better gaming platform than others?\n",
    "Output: request_attribute(has_mac_release[])\n",
    "\n",
    "Note: you are to output the string after \"Output: \". Do not include \"Output: \" in your answer.\n",
    "\n",
    "Give the output for the following sentence:\n",
    "{input}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Replicate client\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Replicate client\n",
    "print(\"Setting up the Replicate client...\")\n",
    "replicate_client = replicate.Client(api_token=os.environ[\"REPLICATE_API_TOKEN\"])\n",
    "print(\"Replicate client set up successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions using Replicate\n",
    "def make_predictions(model_name: str, input_dict: dict, in_dataset: Dataset, prefix: str, out_dir: str, save_intermediate: bool = False):\n",
    "    \"\"\"\n",
    "    Generate predictions using a specified model via Replicate API.\n",
    "    \n",
    "    Args:\n",
    "    - model_name (str): Name of the model on Replicate\n",
    "    - input_dict (dict): Dictionary containing model input parameters\n",
    "    - in_dataset (Dataset): Input dataset\n",
    "    - prefix (str): Prefix for output files\n",
    "    - out_dir (str): Directory to save output files\n",
    "    - save_intermediate (bool): Whether to save intermediate results\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Dictionary containing all responses\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Starting predictions for {model_name}\")\n",
    "    print(f\"Total examples: {len(in_dataset)}\")\n",
    "    responses_dict = {}\n",
    "    for i in range(len(in_dataset)):\n",
    "        response = \"\"\n",
    "        print(i)\n",
    "        if (i % 50) == 0:\n",
    "            print(f\"Processing example {i} of {len(in_dataset)}\")\n",
    "            if save_intermediate:\n",
    "                with open(f\"{out_dir}/{prefix}_responses_dict_{i}.json\", \"w\") as f:\n",
    "                    json.dump(responses_dict, f)\n",
    "    \n",
    "        # Get the text and ground truth for the example\n",
    "        text = in_dataset[i][\"text\"]\n",
    "        ground_truth = in_dataset[i][\"attributes\"]\n",
    "        \n",
    "        # Generate the prompt\n",
    "        prompt = PROMPT_TEMPLATE.format(input=text)\n",
    "        input_dict['prompt'] = prompt\n",
    "\n",
    "        # Generate the response\n",
    "        for event in replicate_client.stream(model_name,input=input_dict):\n",
    "            response += str(event)\n",
    "        \n",
    "        # Store the response in the dictionary\n",
    "        responses_dict[i] = {\"text\": text, \"ground_truth\": ground_truth, \"output\": response}\n",
    "        \n",
    "    return responses_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and save responses for the validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up output directory\n",
    "out_dir = \"responses\"\n",
    "if not os.path.exists(out_dir): os.makedirs(out_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "# Uncomment the desired model configuration\n",
    "\n",
    "# Mixtral 8x7B configuration\n",
    "input_dict = {\n",
    "    \"top_p\": 0.95,\n",
    "    \"prompt\": \"\",\n",
    "    \"max_tokens\": 256,\n",
    "    \"temperature\": 0.7,\n",
    "    \"prompt_template\": \"<s>[INST] {prompt} [/INST] \",\n",
    "}\n",
    "\n",
    "model_name = \"mistralai/mixtral-8x7b-instruct-v0.1\"\n",
    "prefix = \"mistral-8x7B\"\n",
    "\n",
    "# # Llama 3 configuration (same for both 8B and 70B models)\n",
    "# input_dict = {\n",
    "#     \"top_p\": 0.9,\n",
    "#     \"prompt\": \"\",\n",
    "#     \"max_new_tokens\": 256,\n",
    "#     \"temperature\": 0.75,\n",
    "#     \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "# }\n",
    "\n",
    "# # # Llama 3 70B configuration\n",
    "# # model_name = \"meta/meta-llama-3-70b-instruct\"\n",
    "# # prefix = \"llama3-70B\"\n",
    "\n",
    "# # Llama3 8B\n",
    "# model_name = \"meta/meta-llama-3-8b-instruct\"\n",
    "# prefix = \"llama3-8B\"\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "print(\"Generating predictions...\")\n",
    "responses_dict = make_predictions(model_name, \n",
    "                                  input_dict, \n",
    "                                  val_dataset, \n",
    "                                  prefix, \n",
    "                                  out_dir,\n",
    "                                  False\n",
    "                                  )\n",
    "\n",
    "# save the responses to a file for future reference\n",
    "with open(f\"{out_dir}/{prefix}_responses_viggo_val.json\", \"w\") as f:\n",
    "    json.dump(responses_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
